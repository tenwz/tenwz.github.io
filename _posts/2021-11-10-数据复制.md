---
layout: post
title: 数据复制概念略要 
---

通过数据复制系统我们想要达到的目的

1. 容错，当部分组件出现故障，系统依然可以继续工作
2. 提高吞吐量，降低因为大量请求访问所带来的压力
3. 通过在地理位置上更接近用户，降低访问延迟

然而，想要达到以上目的会带来很多技术挑战。

---



由于业务在发展，数据也在持续更改，当有了多副本，不可避免的会引入一些问题，

- 如何确保所有副本之间的数据是一致的。
- 如何确保新的从节点和主节点保持数据一致
- 如果发生主、从节点失效，合适的解决方案是什么
- 如何选择合适的复制方式（日志）
- 如何保证最终一致性 -> 写后读一致性 -> 单调读一致性 -> 前缀读一致性 

---

## 如何确保所有副本之间的数据是一致的。



目前常见的解决方案是采用主从复制，主从复制的工作原理如下：

...

对于复制一个非常重要的考量是采用同步复制还是异步复制

同步复制的优点是...，

但这样做的副作用相当大，第一，性能差。主库必须在等到所有备库均返回成功后，才能向客户端反馈提交成功，如果备库发生<u>网络阻塞</u>，主库的响应时间取决于两个备库中延时最长的那个。第二，可用性问题。任何设备都有出现故障的可能，在同步复制模式下，集群中的节点被<u>串联</u>起来，如果单机可用性是 95%，那么集群整体的可用性是比例的幂次方，与单机相比反而降低了。

在实际业务中，...

实践中，如果数据库启用了同步复制，往往指的是，....，这种配置有时也称为<u>半同步复制</u>

异步复制的优点是，...，

这种弱化的持久性听起来像是一个非常不靠谱的这种设计，但....还是被广泛使用



---

## 如何确保新的从节点和主节点保持数据一致

当如果出现以下情况：

- 增加副本数以提高容错能力
- 替换失败的副本

这时需要考略增加新的从节点，那么如何保证新的从节点和主从点保持数据一致

由于客户端可能仍然不断的向数据库写入数据，即数据始终处于不断变化之中。所以常规的文件拷贝方式会导致不同的节点上会呈现不同时间点的数据。

我们往往采用<u>一致性快照+数据更改日志</u>的方式来保证一致性，逻辑的主要操作如下：

...

---

## 如果发生主、从节点失效，合适的解决方案是什么

#### 从节点失效：追赶式恢复

#### 主节点失效：节点切换

选择某个从节点将其提升为主节点；客户端也应需要更新，这样之后的写请求会发送给新的主节点；其他从节点需要接收来自新的主节点的变更数据。此过程称为<u>切换</u>

故障切换有手动方式和自动方式。

手动方式如通知管理员主节点发生失效，采取必要的步骤来创建新的主节点。

自动方式的切换步骤如下：

1. 确认主节点失效。
2. 选举新的节点
3. 重新配置系统使主节点生效

确认主节点生效。有很多出错原因，如系统崩溃、停电、网络故障。没有万无一失的方法能够确切的检测问题会出在哪里，所以大多数系统都采用了基于<u>超时</u>的机制。节点间定时互相发送<u>心跳存活消息</u>，如果发现某节点在某一时间段中无响应，即认为该节点失效。

选举新节点。通过选举的方式（超过多数的节点达成共识）来选举新的主节点，或者由之前的选定的某控制节点来制定新的主节点。让所有节点同意新的主节点是<u>典型的共识问题</u>。

重新配置系统使新主节点生效。客户端需要将写请求发送给新的主节点（<u>请求路由</u>）。如果原主节点之后重新上线，可能仍然自以为是主节点，而没有意识到其他节点已经达成共识迫使其下台。此时系统应确保原主节点降级为从节点，并认可新的主节点。

尽管如此，自动切换过程仍充斥着许多变数。

- 如果在数据库之外有其他系统依赖于数据库的内容并在一切系统使用，丢弃数据的方案带来的风险就会放大。
- 脑裂。有些系统往往采取措施强制关闭其中一个节点，然而如果设计或实现考虑不周，可能会出现两个节点都被关闭。
- 如何设置合适的超时时间。超时时间过长意味着总体恢复时间过长。超时时间过短意味着可能会导致不必要的误判错误切换。





---

## 如何选择合适的复制方式（日志）

- 基于语句

- 基于预写日志（我不喜欢）
- 基于行的逻辑日志

---

## 复制滞后问题

#### 状态一致性：数据所处的客观、实际状态所体现的一致性

#### 操作一致性：外部用户通过协议约定的操作，能够读取到的数据一致性

从状态的视角来看，任何变更操作<u>后</u>（变更操作要么成功，要么失败），数据只有两种状态，所以副本一致或者不一致。

在某些条件下，不一致的状态只是暂时的，还会转换到一致的状态。

所以在习惯上，大家将<u>暂时</u>的不一致成为“弱一致”，相对的，一致就称之为“强一致”。

但对于弱一致性此时的定义是模糊的，所谓的“暂时”到底是多久呢？

此时就需要我们从操作视角的角度来分析了。

弱一致性在语义上包含了很大的不确定性，很多时候并不能直接使用。所以我们加入一些限定条件，因此也就衍生出很多一致性模型。从操作视角分析的意思便是，在副本不一致的情况下，通过在操作层面的封装，来分析不同的一致性模型对外界表现数据的状态。

下面举五个常见的一致性模型来进行分析

- 写后读一致性（Reading Your Own Writes）

- 单调读一致性（Monotonic Reads)

- 前缀一致性(Consistent Prefix Reads)

- 线性一致性

- 因果一致性

**用一致性强度来衡量的话：线性一致性强于因果一致性；而写后读一致性、单调读一致性、前缀一致性弱于前两者，但这三者之间无法比较强弱。还有一种常被提及的顺序一致性（Sequentially Consistent），其强度介于线性一致性与因果一致性之间，由于较少在分布式数据库中使用，所以并没有介绍。**





写后读一致性是指...

基于主从复制的系统的可行方案有

- 如果用户可能会被修改的内容，从主节点读取；否则在从节点读取。举个例子，社交网络上的用户首页信息通常只能由所有者编辑，那么我们可以：总是从主节点读取用户自己的首页配置文件，而在从节点读取其他用户的配置文件
- 跟踪最近更新的时间。比如，如果修改的内容在更新后一分钟之内，则总是在主节点读取；并监控从节点的复制滞后程度，避免从那些滞后时间超过一分钟的从节点读取。
- 记住客户端最近更新时的时间戳，并附在读请求中。系统可以根据此信息，确保对该用户提供读服务时，都应该至少包含了该时间戳的更新。如果不够新，可以交给另一个副本处理，也可以等待到该副本接收到了最近的更新，时间戳可以是逻辑时间戳（比如用来指示写入顺序的日志序列号）或实际系统时间。
- 跨设备的写后读一致性 ...

单调读是指...

实现单调读的一种方式时，确保每个用户总是从固定的同一副本执行读取，而不同的用户可以从不同的副本读取。例如，基于用户 ID 哈希。如果该副本发生失效，则用户的查询必须重新路由到另一个副本。

前缀一致读是指..



---

## 无主节点复制

对于某些无主节点系统实现，客户端直接将其写请求发送给多副本；而在其它一些实现中，由一个<u>协调者节点</u>代表客户端进行写入，且协调者并不负责对<u>写入顺序</u>的维护。





---

## 两阶段提交



> **Atomicity: Either all the changes from the transaction occur (writes, and messages sent), or none occur.**

原子性要求事务只有两种状态：成功，所有操作都成功。失败，任何操作都没有执行，即使已经执行了部分操作，也要保证回滚这些操作。



原子性提交协议有不少，按照其作用范围可以分为

面向应用层，面向资源层。

面向应用层中比较典型的协议是 TCC （Try，Confirm，Cancel ）协议



TCC 仅是应用层的分布式事务框架，具体操作完全依赖于业务编码实现，可以做针对性的设计，但是这也意味着业务侵入会比较深。

;; 事务管理器有点像 Erlang/OTP 中的 Supervisor

此外，考虑到网络的不可靠，操作指令必须能够被重复执行，这就要求 Try、Confirm、Cancel 必须是幂等性操作，也就是说，要确保执行多次与执行一次得到相同的结果。显然，这又增加了开发难度。



面向资源层中的典型协议 2PC （Two-Phase Commit)

2PC 的处理过程分为准备和提交两个阶段，每个阶段都由<u>协调者</u>和<u>参与者</u>共同完成。

协调者一般是事务管理器，而参与者一般是资源管理器，也就是相应的数据库实例。

准备阶段：协调者向所有参与者发送待执行的程序语句，并询问是否做好提交事务的准备。每个参与者收到请求后，<u>记录下执行日志，锁定好相关事务资源，检查并做出是否可以执行的应答。</u>协调者收到可以执行的反馈，准备阶段结束。

提交：如果所有参与者的反馈是可以执行，那么协调者则会发出提交的指令，参与者接受指令后，会进行本地操作执行程序语句，最后向协调者发送执行成功的响应，事务结束；如果协调者发现某个参与者不具备可以执行语句的条件，则只能向所有参与者发送回滚指令。参与者接受到指令后，撤销准备阶段的操作，释放锁定的资源，并向协调者发送回滚成功的响应，事务结束。

可以看出，2PC的本质其实是：

相比于 TCC，2PC 的优点是借助了数据库的提交和回滚操作，不侵入业务逻辑。但是，它也存在一些明显的问题：

<u>同步阻塞</u>：执行过程中，数据库要锁定对应的数据行。如果其他事务刚好也要操作这些数据行，那它们就只能等待。其实同步阻塞只是设计方式，真正的问题在于这种设计会导致分布式事务出现高延迟和性能的显著下降。

<u>单点故障</u>：协调者事务管理器的角色非常重要，一旦发生故障，数据库会一直阻塞下去。尤其是如果在提交阶段发生故障，所有数据库处于锁定事务资源的状态，从而无法继续完成事务操作。

<u>数据不一致</u>：提交阶段，如果协调者向参与者发送提交请求之后，发生了局部网络异常，那么可能会发生只有部分数据库接收到请求，而其他数据库未能接受到请求，从而无法提交事务，导致整个系统就会出现数据不一致性的现象。

因为存在这些问题，多数分布式数据库都是在 2PC 协议基础上改进，来保证分布式事务的原子性。

这里介绍两种 2PC 改进模型：Percolator 和 PGXC

#### Percolator 				;; 待细化

使用 Percolator 模型的前提是事务的参与者（数据库实例）要支持 MVCC 。

准备阶段，协调者向参与者发送包含了相关的程序执行语句的准备请求。参与者接收到请求后记录执行语句日志，预执行该语句，在修改的记录后添加私有版本。什么是添加私有版本呢？

我们可以简单地构造一个行记录模型：每条行上不仅有我们的业务数据字段，还有相应的锁字段。在锁字段上写入了标识信息的行记录就是私有版本。在 Percolator 模型中，这条记录只有当前写入标识信息的事务才可以访问，其他事务不可以读写。

每个参与者在私有版本上的标识内容并不相同，协调者（事务管理器）会随机选择一个参与者，在其记录上的锁字段中写入标识信息，标记该记录为主锁。其他参与事务的记录会在锁字段中记录指向该主锁记录的指针。

准备阶段结束后，每个参与者都增加了私有版本记录。

提交阶段，协调者只需要和拥有主锁的参与者通讯，发送提交指令。

该参与者执行程序指令，并抹去准备阶段写入的主锁记录，此时该记录不再是私有版本，所有事务可以访问，事务结束。对于其他参与者来说，当其他事务读取到相应的记录时，会根据指针查找到主锁记录，发现记录已经提交。所以此时该记录虽然是私有版本格式，仍然可以认为已经生效。当然，如果每个事务都在进行该操作，那么势必会造成较大的性能损耗。所以 Percolator 引入了其他异步线程来更新相关参与者的记录，抹去私有版本信息。

对于数据不一致问题，事务管理器只需要和一个参与者通讯，由于该提交操作本身是原子的，所以事务的状态自然也是原子的，数据不一致问题被完美解决了。

对于单点故障问题，Percolator 引入的异步线程可以在事务管理器宕机后，回滚各个分片上的事务，提供了善后手段，不会让分片上被占用的资源无法释放。另外，事务管理器可以用记录日志的方式使自身无状态化，日志通过共识算法同时保存在系统的多个节点上。这样，事务管理器宕机后，可以在其他节点启动新的事务管理器，基于日志恢复事务操作。

---

#### 持久性

持久性的核心思想是是应对系统故障，一般来说，系统故障分为两种：

存储硬件无损可以恢复的故障。这种情况下我们通过预写日志来保证第一时间存储数据。值得说道的是，预写日志往往采用顺序写入的方式，来保证数据库的低延时响应。

存储硬件损坏，不可恢复的故障。这种情况我们需要用到日志复制技术，将本地日志及时同步到其他节点。实现方式大体有三种：单体数据库自带的同步和半同步的方式。第二种是将日志存储到共享存储系统上，后者通过冗余存储保证日志的安全性，业界有名的存储系统是 Share Storage。第三种是基于Paxos/Raft的共识算法同步日志数据。

#### 分片

分片策略，主要有哈希 Hash 和 范围 Range 两种。分片的调度机制分静态和动态两种，静态意味着分片在节点上的分布基本是固定的，动态则是指通过调度管理器基于算法在各节点之间自动地移动分片。

直接通过数据集特征哈希到到不同分片的好处是保证数据可以均匀分布，但是劣势在于如果系统节点数量变动，数据就要重新哈希计算，从而带来大规模的数据迁移，不利于扩展性。一个提升扩展性的方法是一致性哈希算法，这里不赘述。

与Hash分片不同，Range分片的特点在于能够加入对于业务的预估。

Range 动态分片多数采用主键作为关键字分片，因为主键可以是系统自动生成，也可以是用户指定。如果采用了用户指定主键的方式，那么理论上可以通过设定主键的产生规则，控制数据流向哪个分片。但是由于主键必须保证唯一性，甚至是单调递增的，那么会导致这种控制比较复杂，使用成本很高。所以我们基本可以认为，分片是一个系统自动处理的过程，用户感知不到。

当单个分片的数据量超过设定值时，分片可以一分为二，这样就可以保证每个分片的数据较为均衡。多个数据量较少的分片，多个数据量较少的分片会在一定的周期内被合并为一个分片。

Range 动态分片也可以根据访问压力调度分片

Spanner 在分片之下增加了目录结构，作为数据调度的最小单位，它的调度可以跨分区，通过调度目录可以将频繁参与同样事务的数据转移到同一个分片下，从而将分布式事务转换为本地事务。

另外，Spanner 也可以通过将目录调度到靠近用户的数据中心，缩短数据的传输时间。当然这里的调度对象是数据的祝福本，跨中心的数据副本依然存在，负责保证系统整体的高可靠性。

尽管目录结构带来了新的特性但也削弱了分片的原有功能，分片内的记录不再连续，扫描要付出更大的成本。但

减少分布式事务，靠近客户端位置本身就是不能兼顾的，再加上存储和访问压力，分片调度机制要在四个目标间进行更复杂的权衡。

#### 数据库的基本架构

数据库在逻辑上可以拆分为五个部分

- 客户端通讯
- 查询处理 -> Parser -> Query Rewrite ->  QueryOptimizer
- 事务存储
- 进程管理
- 共享组件与工具

```scheme
(define true (lambda (x y) x))
(define false (lambda (x y) y))
(define if-then-else (lambda (p x y) (p x y)))
```

用丘奇数表示自然数

有一个函数 zero？，当其参数为0时返回T，否则返回F

有一个减一函数sub1，参数为0时返回0，参数为其他自然数时，将该自然数减一后返回

定义

```scheme
(>= x y)
```

可以利用 zero？和 sub1 来判断对y进行x次“减一”的值是否是0

则（> x y) == 































































































